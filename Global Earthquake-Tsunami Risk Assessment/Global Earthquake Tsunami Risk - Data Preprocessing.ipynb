{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4af7ab22-ebf1-4847-b107-193edef82e51",
   "metadata": {},
   "source": [
    "# Global Earthquake-Tsunami Risk Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d6a06a-2abb-4738-8b80-b70d0628a9ce",
   "metadata": {},
   "source": [
    "## Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43b5d5fd-731e-43b9-9761-45aac921d503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5fdb6f-e887-4c40-853c-2606e5c35f80",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5271d884-f4da-4d02-a268-4d5e206e70cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_data = pd.read_csv('https://raw.githubusercontent.com/AbhishekBiswas-github/Python_Data_Science/refs/heads/main/Global%20Earthquake-Tsunami%20Risk%20Assessment/earthquake_data_tsunami.csv')\n",
    "initial_data = np.array(initial_data)\n",
    "unscaled_data = initial_data[:,:-3]\n",
    "targets_data = initial_data[:,-1:].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314577bc-1ce3-402e-8b16-890bcff48705",
   "metadata": {},
   "source": [
    "## Balancing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "819bc081-afd9-4dee-b450-5b445232ae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_one_targets = np.sum(targets_data.flatten() == 1)\n",
    "num_zero_targets = 0\n",
    "indicies_deletion = []\n",
    "\n",
    "for i in range(targets_data.shape[0]):\n",
    "    if targets_data[i] == 0:\n",
    "        num_zero_targets += 1\n",
    "        if num_zero_targets > num_one_targets:\n",
    "            indicies_deletion.append(i)\n",
    "\n",
    "unscaled_balanced_input_data = np.delete(unscaled_data, indicies_deletion, axis=0)\n",
    "balanced_target_data = np.delete(targets_data, indicies_deletion, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e33c63-703a-4615-bae0-91d96e70c988",
   "metadata": {},
   "source": [
    "## Standardizing the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d1e00e8-ddd6-4e53-a706-bbebf5f691e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_inputs = preprocessing.scale(unscaled_balanced_input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb67e9c-fd94-4b03-af6d-35af7ac3a0e3",
   "metadata": {},
   "source": [
    "## Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e880839b-da1a-4a2c-a71d-9c063a6304b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "shuffled_input = scaled_inputs[shuffled_indices]\n",
    "shuffled_target = balanced_target_data[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4237292-b575-4d2b-a080-3fd3e47f2d46",
   "metadata": {},
   "source": [
    "## Split the dataset into train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04955847-c5c6-404c-9ba2-423bc443cb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248 486 0.5102880658436214\n",
      "32 60 0.5333333333333333\n",
      "24 62 0.3870967741935484\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_COUNT = shuffled_input.shape[0]\n",
    "\n",
    "train_size = int(0.8 * SAMPLE_COUNT)\n",
    "validation_size = int( 0.1 * SAMPLE_COUNT)\n",
    "test_size = SAMPLE_COUNT - train_size - validation_size\n",
    "\n",
    "train_input_data = shuffled_input[:train_size]\n",
    "train_target_data = shuffled_target[:train_size]\n",
    "\n",
    "validation_input_data = shuffled_input[train_size: train_size + validation_size]\n",
    "validation_target_data = shuffled_target[train_size: train_size + validation_size]\n",
    "\n",
    "test_input_data = shuffled_input[train_size + validation_size:]\n",
    "test_target_data = shuffled_target[train_size + validation_size:]\n",
    "\n",
    "print(np.sum(train_target_data), train_size, np.sum(train_target_data) / train_size)\n",
    "print(np.sum(validation_target_data), validation_size, np.sum(validation_target_data) / validation_size)\n",
    "print(np.sum(test_target_data), test_size, np.sum(test_target_data) / test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa87405-8d6f-4f20-a14b-0b8f726c7e5a",
   "metadata": {},
   "source": [
    "## Saving the datasets in tensorflow format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2685452-60bc-45a2-9374-b86c4afbf564",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"Earthquake_data_train\", inputs=train_input_data, targets=train_target_data)\n",
    "np.savez(\"Earthquake_data_validate\", inputs=validation_input_data, targets=validation_target_data)\n",
    "np.savez(\"Earthquake_data_test\", inputs=test_input_data, targets=test_target_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
